import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Concatenate, Add
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error
from skimage.metrics import structural_similarity as ssim
from tensorflow.keras.applications import VGG19
from tensorflow.keras.models import Model

def residual_block(x, filters):
    shortcut = x
    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    x = Add()([shortcut, x])
    return x

def build_generator():
    input_layer = Input(shape=(256, 256, 3))

    # Encoder
    e1 = Conv2D(64, (4, 4), strides=(2, 2), padding='same')(input_layer)
    e2 = LeakyReLU(alpha=0.2)(e1)
    e3 = Conv2D(128, (4, 4), strides=(2, 2), padding='same')(e2)
    e4 = BatchNormalization()(e3)
    e5 = LeakyReLU(alpha=0.2)(e4)
    e6 = Conv2D(256, (4, 4), strides=(2, 2), padding='same')(e5)
    e7 = BatchNormalization()(e6)
    e8 = LeakyReLU(alpha=0.2)(e7)

    # Residual blocks
    r = residual_block(e8, 256)
    r = residual_block(r, 256)
    r = residual_block(r, 256)
    r = residual_block(r, 256)

    # Decoder
    d1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(r)
    d2 = BatchNormalization()(d1)
    d3 = Concatenate()([d2, e5])
    d4 = LeakyReLU(alpha=0.2)(d3)
    d5 = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(d4)
    d6 = BatchNormalization()(d5)
    d7 = Concatenate()([d6, e2])
    d8 = LeakyReLU(alpha=0.2)(d7)
    d9 = Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same')(d8)
    output_layer = tf.keras.activations.tanh(d9)

    return Model(inputs=input_layer, outputs=output_layer)

def build_discriminator():
    input_layer = Input(shape=(256, 256, 3))
    target_layer = Input(shape=(256, 256, 3))

    merged_layer = Concatenate()([input_layer, target_layer])

    d1 = Conv2D(64, (4, 4), strides=(2, 2), padding='same')(merged_layer)
    d2 = LeakyReLU(alpha=0.2)(d1)
    d3 = Conv2D(128, (4, 4), strides=(2, 2), padding='same')(d2)
    d4 = BatchNormalization()(d3)
    d5 = LeakyReLU(alpha=0.2)(d4)
    d6 = Conv2D(256, (4, 4), strides=(2, 2), padding='same')(d5)
    d7 = BatchNormalization()(d6)
    d8 = LeakyReLU(alpha=0.2)(d7)
    d9 = Conv2D(512, (4, 4), strides=(1, 1), padding='same')(d8)
    d10 = BatchNormalization()(d9)
    d11 = LeakyReLU(alpha=0.2)(d10)
    output_layer = Conv2D(1, (4, 4), strides=(1, 1), padding='same')(d11)

    return Model(inputs=[input_layer, target_layer], outputs=output_layer)

def build_vgg():
    vgg = VGG19(weights="imagenet", include_top=False, input_shape=(256, 256, 3))
    vgg.trainable = False
    model = Model(inputs=vgg.input, outputs=vgg.get_layer("block5_conv4").output)
    return model

def build_gan(generator, discriminator, vgg):
    discriminator.trainable = False
    input_layer = Input(shape=(256, 256, 3))
    generated_image = generator(input_layer)
    gan_output = discriminator([input_layer, generated_image])
    perceptual_output = vgg(generated_image)

    return Model(inputs=input_layer, outputs=[gan_output, generated_image, perceptual_output])

generator = build_generator()
discriminator = build_discriminator()
vgg = build_vgg()
gan = build_gan(generator, discriminator, vgg)

discriminator.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='mse')
gan.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss=['mse', 'mae', 'mse'], loss_weights=[1, 100, 10])

def load_data(path_to_images, path_to_labels):
    images = []
    labels = []
    for img_name in os.listdir(path_to_images):
        img_path = os.path.join(path_to_images, img_name)
        label_path = os.path.join(path_to_labels, img_name)
        if os.path.exists(label_path):  # Ensure corresponding target image exists
            img = cv2.imread(img_path)
            img = cv2.resize(img, (256, 256))
            images.append(img)
            label = cv2.imread(label_path)
            label = cv2.resize(label, (256, 256))
            labels.append(label)
    images = np.array(images).astype(np.float32)
    labels = np.array(labels).astype(np.float32)
    images = (images - 127.5) / 127.5
    labels = (labels - 127.5) / 127.5
    return images, labels

path_to_underwater_images = '/content/drive/MyDrive/low'
path_to_clean_images = '/content/drive/MyDrive/high'
train_data, target_data = load_data(path_to_underwater_images, path_to_clean_images)

# Assuming we have separate validation data
val_low_images, val_high_images = load_data('/content/drive/MyDrive/val_low', '/content/drive/MyDrive/val_high')

epochs = 100
batch_size = 32
for epoch in range(epochs):
    for i in range(len(train_data) // batch_size):
        idx = np.random.randint(0, len(train_data), batch_size)
        imgs, target_imgs = train_data[idx], target_data[idx]

        generated_imgs = generator.predict(imgs)

        d_loss_real = discriminator.train_on_batch([imgs, target_imgs], np.ones((batch_size,) + discriminator.output_shape[1:]))
        d_loss_fake = discriminator.train_on_batch([imgs, generated_imgs], np.zeros((batch_size,) + discriminator.output_shape[1:]))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        perceptual_target = vgg.predict(target_imgs)
        g_loss = gan.train_on_batch(imgs, [np.ones((batch_size,) + discriminator.output_shape[1:]), target_imgs, perceptual_target])

        print(f"{epoch} [D loss: {d_loss}] [G loss: {g_loss}]")

    # Evaluate model performance
    generated_images = generator.predict(val_low_images)
    mse = mean_squared_error(val_high_images.flatten(), generated_images.flatten())
    ssim_value = ssim(val_high_images, generated_images, multichannel=True)
    print(f"{epoch} [MSE: {mse}] [SSIM: {ssim_value}]")


#here
    enhanced_val_folder_path = f'/content/drive/MyDrive/val_enhanced_epoch_{epoch}'
    if not os.path.exists(enhanced_val_folder_path):
        os.makedirs(enhanced_val_folder_path)

    for idx, img_name in enumerate(os.listdir('/content/drive/MyDrive/val_low')):
        enhanced_image = generated_images[idx]
        enhanced_image = (enhanced_image * 127.5 + 127.5).astype(np.uint8)  # Denormalize
        cv2.imwrite(os.path.join(enhanced_val_folder_path, img_name), enhanced_image)

def enhance_image_folder(input_folder_path, output_folder_path):
    if not os.path.exists(output_folder_path):
        os.makedirs(output_folder_path)

    for img_name in os.listdir(input_folder_path):
        input_image_path = os.path.join(input_folder_path, img_name)
        output_image_path = os.path.join(output_folder_path, img_name)

        image = cv2.imread(input_image_path)
        image = cv2.resize(image, (256, 256))
        image = (image - 127.5) / 127.5

        enhanced_image = generator.predict(np.expand_dims(image, axis=0))[0]
        enhanced_image = (enhanced_image * 127.5 + 127.5).astype(np.uint8)  # Denormalize

        cv2.imwrite(output_image_path, enhanced_image)

input_folder_path = '/content/drive/MyDrive/low'
output_folder_path = '/content/drive/MyDrive/enhanced'

enhance_image_folder(input_folder_path, output_folder_path)
